---
title: Interactional Epistemics (Standalone)
---
 
## A Permeability Criterion for Scientific and Philosophical Models

---

## Abstract

Scientific and philosophical models are often treated as closed systems whose internal consistency and empirical success are taken as indicators of truth. This paper argues that such closure is best understood as a practical abstraction rather than an ontological fact. Because empirically studied systems exist within interacting environments, any model that claims to describe reality must remain open to interactions beyond its explicit scope.

I propose the **Interactional Permeability Principle**: a model that aims to describe empirical reality is epistemically legitimate only if it admits its openness to interactions from known factors, known unknowns, and unknown unknowns. Persistent mismatches between model and observation are interpreted as constraints signaling unscoped structure rather than mere noise.

This framework is grounded in a combinatorial argument: modeling requires enumerating and partitioning empirical structure, and the space of possible partitions grows explosively. Multiple inequivalent models can therefore match the same finite dataset. Permeability becomes not merely a philosophical recommendation but a structural necessity.

Interactional Epistemics shifts epistemology from the pursuit of closed, final truths to the mapping of open, interacting localities. It offers a criterion for distinguishing strategically bounded models from dogmatically closed ones and reframes scientific progress as adaptive compression under constraint.

---

## 1. Introduction

Scientific models simplify aspects of reality for analysis. They isolate variables, ignore interactions, and operate under idealized assumptions. Such simplifications are indispensable: without them, calculation and prediction would be intractable.

A recurring epistemic error occurs when practical simplifications are treated as ontological conclusions. Operational closure—introduced for tractability—becomes mistaken for isolation in reality.

Historically, models have been defended despite accumulating anomalies:

- Economic models that bracket ecological constraints  
- Classical mechanics applied beyond its relativistic domain  
- Early cosmological models prior to resolving galactic rotation discrepancies  

In each case, practical limits were implicitly treated as statements about the structure of reality itself.

This paper proposes a criterion designed to prevent such dogmatic closure:

> Any model that claims to describe empirical reality must admit that it is open to interactions beyond its scope.

This is the **Interactional Permeability Principle**.

---

## 2. The Problem of Ontological Closure

### 2.1 Traditional Criteria of Scientific Legitimacy

Scientific models are typically evaluated by:

- Internal logical consistency  
- Empirical adequacy  
- Predictive success  

These criteria are necessary but not sufficient. A model may satisfy all three while still:

- Ignoring interacting domains  
- Treating idealizations as literal structure  
- Deflecting persistent mismatches as noise  

This produces **epistemic brittleness**: the model performs well within a narrow scope but fails under expanded interaction.

---

### 2.2 Strategic vs Ontological Closure

It is crucial to distinguish two types of closure:

**Strategic Closure**  
- Boundaries are explicitly declared.  
- Scope limitations are acknowledged.  
- Simplifications are recognized as operational.  

**Ontological Closure**  
- Boundaries are treated as real features of reality.  
- Excluded variables are treated as nonexistent.  
- Model success is mistaken for structural exhaustiveness.  

Strategic closure is necessary for finite agents.  
Ontological closure is unjustified unless interactional exhaustiveness is demonstrated.

The burden of proof lies on any claim of interactional completeness.

---

### 2.3 Distinction from *Ceteris Paribus*

The Interactional Permeability Principle must be distinguished from the classical methodological device of *ceteris paribus* (“all else equal”).

*Ceteris paribus* functions as a **local bracketing operator**. It temporarily freezes background variables in order to analyze a focal relationship. It is a practical necessity for tractable reasoning. When used properly, it makes no ontological claim that the bracketed factors do not exist.

Interactional Permeability operates at a different level.

It does not govern how variables are bracketed within a model.  
It governs how bracketing itself is interpreted epistemically.

The distinction can be summarized as follows:

- *Ceteris paribus* allows temporary isolation for analysis.  
- Interactional Permeability forbids treating that isolation as evidence of ontological exhaustiveness.  

*Ceteris paribus* assumes that “other things” exist but are being held fixed.  
Interactional Permeability insists that those “other things” may include:

- Known interacting variables  
- Known unknowns  
- Structurally unenumerated factors  
- Entirely new partitions of structure  

The former is a modeling convenience.  
The latter is a constraint on epistemic posture.

A model may legitimately employ *ceteris paribus* conditions.  
It becomes dogmatic only when:

- The bracket is mistaken for reality itself, or  
- The hypothesis space is treated as interactionally complete without demonstration.  

Interactional Permeability therefore does not replace *ceteris paribus*.  
It regulates how and when its use becomes epistemically illegitimate.

---

### 2.4 The Interactional Condition

Empirically studied systems are not perfectly isolated. They exist within environments in which they exchange energy, matter, or information and are embedded within broader causal structures.

Accordingly, any model describing such a system is:

- A local approximation  
- Embedded within a larger interacting context  

Models may be operationally closed for calculation, but should not be treated as ontologically closed in their claims about reality.

This is not a metaphysical claim about the universe. It is a methodological constraint governing how empirical models should be held.

---

## 3. The Combinatorial Argument for Permeability

The need for permeability follows from structural features of modeling itself.

### 3.1 Modeling Requires Enumeration

Empirical data is not given as pre-packaged objects. It consists of continuous, interacting, high-dimensional processes. To construct a model, we must:

- Select variables  
- Partition data  
- Define objects  
- Establish relations  
- Identify invariances  
- Discard information  

Each of these steps is an act of **enumeration and partitioning**.

The world does not arrive with boundaries labeled. Objecthood is stabilized through interaction and compression.

---

### 3.2 Enumeration Is Combinatorial

For any sufficiently complex dataset, the number of possible ways to:

- Partition it  
- Group it into objects  
- Assign relations  
- Select invariances  

grows combinatorially.

Multiple structurally distinct models can therefore fit the same finite dataset.

This is not skepticism. It is structural abundance.

---

### 3.3 The Apple Thought Experiment

Consider a simple apple.

How many objects are in it?

- One apple  
- Skin, flesh, core, seeds  
- Millions of cells  
- Trillions of molecules  
- Quantum excitations  

Or we may group non-neighboring regions by:

- Magnetic polarity  
- Orientation  
- Phase  
- Functional role  
- Observer-relative geometry  

There is no uniquely privileged object count given directly by reality.

Instead:

- Reality supplies structured interaction.  
- Partitioning depends on invariance and purpose.  
- Constraints stabilize some partitions and destabilize others.  

The apple illustrates that partitioning space is combinatorial and purpose-relative, yet filtered by constraint. Objecthood is not arbitrary, but neither is it uniquely fixed.

---

### 3.4 Finite Agents and Overfitting

Finite agents cannot enumerate all possible partitions of empirical data.

When modeling:

1. A target phenomenon is selected.  
2. Salient variables are chosen.  
3. A compression is constructed.  
4. Non-salient structure is ignored.  

This is necessary. But it creates structural risk.

Like statistical overfitting, a model may perform perfectly within a limited domain while failing under expanded interaction.

Permeability is therefore not humility for its own sake. It is regularization against inevitable overfitting in combinatorial model space.

---

### 3.5 Permeability and Bayesian Updating

Because modeling requires selective partitioning, model construction implicitly operates over a hypothesis space.

Bayesian updating provides a formal mechanism for redistributing credibility within such a space under empirical constraint. As new data arrives, posterior probability shifts across competing partitions.

In this sense, Bayesian updating operationalizes permeability within a fixed hypothesis class.

However, combinatorial abundance implies that no finite hypothesis space can be guaranteed exhaustive.

Dogmatic closure occurs when:

- Posterior concentration is treated as ontological certainty, or  
- The hypothesis space is prematurely fixed despite persistent anomaly.  

True permeability requires openness both to updating within a model class and to expanding the model class itself when constraint pressure indicates unscoped structure.

---

## 4. The Interactional Permeability Principle

### Core Criterion

A model that claims to describe empirical reality is epistemically legitimate only if:

1. It admits openness to interactions beyond its explicit scope.  
2. It treats excluded factors as unscoped influences rather than nonexistent ones.  
3. It remains updateable under expanding constraint and does not treat its partition of structure as probabilistically exhaustive.  
4. It interprets persistent mismatches as constraints indicating unscoped structure.  

The burden of proof lies on any claim of interactional exhaustiveness.

---

## 5. Constraint, Convergence, and Robustness

### 5.1 Constraint as Structural Detection

Mismatch is not mere error.  
It is signal.

Constraints reveal interaction beyond scope.

An anomaly is structural information.

---

### 5.2 Convergence Under Constraint

Some aspects of modeling are relative to aim and scale. But constraint causes convergence.

Different models may begin from different compressions. Under cross-domain testing, they converge.

Relativity is filtered by interaction.

---

### 5.3 Robustness as Survivability Under Expansion

Robustness is:

- Stability under new interaction  
- Integration across domains  
- Constraint absorption  
- Survival under scope expansion  

Science progresses not by discovering a final enumeration, but by constructing compressions that survive broader exposure.

---

## 6. A Geometric Representation of Understanding

Understanding can be represented heuristically in a two-dimensional plane:

- **X-axis:** Degree of model-to-model integration  
- **Y-axis:** Scope of the domain explained  

Low X, Low Y:  
Isolated, narrow models.

High X, High Y:  
Broad scope, deeply integrated compressions.

Scientific progress often involves expanding scope (upward) and increasing integration (rightward).

This geometric representation is itself a compression—useful pedagogically, not ontologically final.

---

## 7. Explanatory vs Engineering Modeling

### 7.1 Explanatory Modeling

Aims at structural adequacy.

- Expands scope  
- Integrates domains  
- Detects deep constraints  

Permeability is mandatory.

---

### 7.2 Engineering Modeling

Aims at functional reliability.

- Restricts scope  
- Enforces boundary conditions  
- Designs safety margins  

Closure is strategic, but safety factors implicitly acknowledge unscoped interaction.

---

## 8. Non-Teleological Realism

Interactional Epistemics rejects:

- Final enumeration  
- Guaranteed convergence to a single ultimate carving  
- Teleological certainty  

It affirms:

- Structured reality  
- Constraint-driven convergence  
- Expanding domains of stability  

Deep structure, if it exists, appears as invariance across expanding interaction—not as a final object inventory.

---

## 9. Recursive Application: The Epistemic Status of This Framework

### 9.1 Interactional Epistemics Is Itself a Model

This paper does not present a final, immutable law of epistemology. It is a combinatorial compression of modeling practice.

It partitions the complex history of scientific discovery into specific objects ("constraints," "boundaries," "permeability") to render them tractable.

As such, it is operationally closed but must remain ontologically open.

---

### 9.2 The Prediction of Revision

By its own principle, this framework predicts that it will eventually encounter unscoped interactions—philosophical, scientific, or cognitive—that it cannot currently account for.

Dogmatic Outcome:  
If this framework claimed final authority, it would become brittle.

Permeable Outcome:  
By admitting approximation, it anticipates refinement, revision, or absorption into more robust theory.

---

### 9.3 Recursive Legitimacy

Interactional Epistemics derives legitimacy not from finality but from:

- Admitted scope  
- Constraint responsiveness  
- Openness to revision  

It is a local map of how we map the world.

---

## 10. Conclusion

Empirical models are local compressions of interacting structure. They arise through selective enumeration within combinatorially vast possibility spaces.

Because:

- Partition space is combinatorial  
- Structural arrangements are underdetermined by finite data  
- Finite agents cannot guarantee interactional exhaustiveness  

No model can justifiably claim ontological closure.

Scientific progress becomes adaptive stabilization under constraint.

Understanding expands not by finalizing structure, but by surviving broader exposure.

**Final Thesis:**  
A model becomes dogmatic when it denies the possibility that real structure exists beyond its scoped description.

Interactional Epistemics replaces the search for final enumeration with disciplined openness under constraint.

---

## 11. Relation to Existing Frameworks

Interactional Epistemics does not emerge in isolation. It intersects with, extends, and sharpens several established traditions in philosophy of science and epistemology.

This section clarifies those relationships while identifying the specific structural contribution of the Interactional Permeability Principle.

---

### 11.1 Underdetermination (Duhem–Quine)

Pierre Duhem and W. V. O. Quine articulated the thesis that empirical data underdetermines theory: multiple incompatible theoretical frameworks can fit the same observational evidence.

Interactional Epistemics agrees with this diagnosis but grounds it structurally.

Traditional underdetermination arguments are epistemological:
> Evidence alone cannot uniquely determine theory.

Interactional Epistemics provides a combinatorial basis:
- Model construction requires partitioning.
- Partition space grows combinatorially.
- Finite agents cannot enumerate exhaustively.
- Therefore ontological closure is unjustified.

The contribution here is not the claim that theory is underdetermined, but the explanation of *why* underdetermination is structurally inevitable for finite agents operating within combinatorial model spaces.

---

### 11.2 Model Pluralism and Perspectivism

Nancy Cartwright’s model pluralism and Ronald Giere’s scientific perspectivism argue that scientific models are patchwork, perspectival, and domain-relative.

Interactional Epistemics overlaps in rejecting single-model exhaustiveness. However, it diverges in refusing permanent fragmentation.

Pluralism often emphasizes coexistence of models.  
Interactional Epistemics emphasizes **convergence under constraint**.

Models that survive expanding interaction, cross-domain testing, and constraint pressure tend to stabilize. Plurality is expected locally; convergence is expected under sustained exposure.

Thus, this framework is neither monist nor laissez-faire pluralist. It is structurally plural but constraint-filtered.

---

### 11.3 Structural Realism

Structural realism (e.g., John Worrall; James Ladyman) argues that what survives theory change is structural continuity rather than object ontology.

Interactional Epistemics aligns with the emphasis on structure over object inventory. However, the focus differs.

Structural realism highlights mathematical or relational continuity across theory change.  
Interactional Epistemics emphasizes **interactional survivability under scope expansion**.

Structure is not privileged because it is abstract, but because it remains invariant across expanding domains of interaction.

In this sense, the framework may be described as a form of **interactional structural realism**: what is real is what remains stable under expanding constraint.

---

### 11.4 Lakatos and Anomaly Response

Imre Lakatos distinguished between progressive and degenerative research programmes based on how they respond to anomaly.

Interactional Epistemics formalizes this distinction normatively:

Dogmatic closure occurs when:
- Persistent anomaly is treated as noise,
- Auxiliary adjustments are added without expanding model class,
- Or hypothesis space is treated as interactionally exhaustive.

Permeability provides a structural criterion for when boundary defense becomes epistemically illegitimate.

Where Lakatos described historical patterns, Interactional Epistemics articulates a general rule governing closure.

---

### 11.5 Bayesian Epistemology

Bayesian updating formalizes rational belief revision under uncertainty within a defined hypothesis space.

Interactional Epistemics affirms Bayesian updating as a powerful intra-class mechanism.

However, classical Bayesianism presupposes a fixed hypothesis space.

Interactional Permeability introduces an additional requirement:
- Openness not only to posterior revision,
- But to expansion of the hypothesis space itself when constraint pressure indicates unscoped structure.

Thus, permeability extends Bayesian reasoning beyond posterior adjustment to meta-level model class revision.

---

### 11.6 Compression and Information-Theoretic Approaches

Minimum Description Length (MDL), Solomonoff induction, and related compression-based frameworks treat modeling as lossy encoding under resource constraint.

Interactional Epistemics aligns closely with this tradition.

However, it introduces a distinctive emphasis:
- Robustness is measured by survivability under expanding interaction, not solely by descriptive compactness.
- Compression quality is tested by cross-domain exposure and constraint absorption.

The combinatorial argument places compression theory inside an explicitly interactional setting.

---

### 11.7 Kuhn and Paradigm Dynamics

Thomas Kuhn described scientific revolutions as paradigm shifts triggered by anomaly accumulation.

Interactional Epistemics differs in posture.

Kuhn offered a sociological description of how science changes.  
Interactional Epistemics provides a normative principle for when closure becomes unjustified.

Dogmatic insulation is illegitimate from the start if interactional exhaustiveness is not demonstrated.

This framework is therefore prescriptive rather than merely descriptive.

---

### 11.8 Open Systems Theory

Systems theorists and thermodynamic approaches (e.g., Ludwig von Bertalanffy; Ilya Prigogine) emphasize that real systems exchange energy, matter, and information.

Interactional Epistemics generalizes this insight from ontology to epistemology:

If systems are open,
models of those systems must assume interactional permeability.

The principle shifts open-system realism into a constraint on modeling practice.

---

## 11.9 Structural Contribution

Interactional Epistemics does not claim to overturn these traditions.

It synthesizes and sharpens them by grounding fallibilism, pluralism, and structural realism in a combinatorial argument about model space and finite cognition.

Its distinctive claims are:

- Closure requires proof of interactional exhaustiveness.
- Partition space is combinatorially vast.
- Finite agents cannot guarantee exhaustive enumeration.
- Robustness is survivability under scope expansion.
- Bayesian updating is necessary but insufficient without model-class permeability.

The contribution is therefore not a new metaphysics, but a structural discipline governing epistemic posture under constraint.

---

## 11.10 Modesty Clause

Interactional Epistemics is itself a local compression within a combinatorial space of epistemological frameworks.

Its legitimacy depends not on novelty, but on:

- Coherence under constraint,
- Openness to revision,
- And survivability under expanding interaction.

If it fails those tests, it must yield.

Permeability applies recursively.